- VScan can now be used with any OpenAI protocol compatible LLM server and model, making the app fully universal and independent. See the project's readme for more details about the new architecture. Thanks to this change, now it's even possible to use self-hosted models, giving the users a full control over their data and privacy.
- VScan now recognizes 25 common proprietary, open-weight and open-source models, including LLMs from the GPT, Claude, Gemini, Gemma, Llama and Qwen families, while 5 different backends are supported out of the box. The users are indeed free to configure any provider or model they wish to use with the app, as far as it supports the required protocol.
- The main scanning screen undervent a slight redesign. It now features a multipurpose edit field, which by default can be used to send a message to the current conversation, but if the user activates the system prompt or user prompt button on the upper bar, they can also configure these parameters right from the scanning screen. It is still possible to use voice input for these actions, which can be performed by long-pressing the respective buttons, although this feature is not yet fully tuned up.
- Users can now also configure various actions for events such as pressing / long pressing volume buttons or shaking the phone. The actions include capturing a photo to be processed by a config, consulting a config or sending a message to the model by voice. The shaking event is not yet properly supported, a deeper sensor integration is on the roadmap.
- Auto describing saved images is now configurable and switched of by default, just like using the flashlight, in order to avoid surprises.
