<p>
OpenAi's GPT is notoriously known for its good role-playing skills. You can tell it it's a teapod and it will act like a teapod, you can specify exact behavior, output etc. GPT 4 vision is no different in this regard.
</p>

<p>
VScan allows you to tell GPT 4V it's an accessibility aid, or to simply just give it instructions how to behave. Then you can just take pictures, GPT will always act in the way you configured it on the primary screen. If you tell it it's a color detector that should respond by writing down the color your index finger is pointing to, you can afterwards just repeatedly press the Take a picture button and pointing your finger to various items, GPT will respond in style "brown... white... green..." etc.
</p>

<p>
This way, VScan essentially becomes a tool of your liking. The app is also cost-effective, each scan starts a new conversation, limiting used tokens and therefore, the price. Having full control over the system and user prompts also gives you full power to apply the good practices of prompt engineering, so you can experiment and find prompts that produce the most efficient and reliable outputs.
</p>

<p>
If no input configuration is entered, the app uses "What's in the image?" prompt and can be used for general purpose scene recognition. Asking follow up questions functionality is also present regardless of the used configuration.
</p>

<p>
Note: VScan uses api of OpenAI, a third-party service. In order to make it work, you need to optain an api key from <a href="https://platform.openai.com">platform.openai.com</a>. See VScan's readme for more information on instructions and approximate pricing.
</p>

<p>
Icon <a href="https://www.flaticon.com/free-icons/qr-scan" title="qr scan icons">Qr scan icons created by Bharat Icons - Flaticon</a>
</p>
